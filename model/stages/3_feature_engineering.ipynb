{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import dependencies",
   "id": "fd6288d414c9adfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:27:36.889988Z",
     "start_time": "2024-11-13T15:27:36.524598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Manipulations and Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Classifiers\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Model Evaluation Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Tuning & Splitting Libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Services\n",
    "import services.ModelStageService as sds\n",
    "\n",
    "# utils\n",
    "import datetime\n",
    "import utils.EDAUtils as eda_utils\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.color_palette('pastel')\n",
    "\n",
    "stage_data_io_service = sds.ModelStageService(previous_stage_name=sds.PREPROCESSING_STAGE, current_stage_name=sds.FEATURE_ENGINEERING_STAGE)"
   ],
   "id": "fcff937794ab8a13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6313725490196078, 0.788235294117647, 0.9568627450980393),\n",
       " (1.0, 0.7058823529411765, 0.5098039215686274),\n",
       " (0.5529411764705883, 0.8980392156862745, 0.6313725490196078),\n",
       " (1.0, 0.6235294117647059, 0.6078431372549019),\n",
       " (0.8156862745098039, 0.7333333333333333, 1.0),\n",
       " (0.8705882352941177, 0.7333333333333333, 0.6078431372549019),\n",
       " (0.9803921568627451, 0.6901960784313725, 0.8941176470588236),\n",
       " (0.8117647058823529, 0.8117647058823529, 0.8117647058823529),\n",
       " (1.0, 0.996078431372549, 0.6392156862745098),\n",
       " (0.7254901960784313, 0.9490196078431372, 0.9411764705882353)]"
      ],
      "text/html": [
       "<svg  width=\"550\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#a1c9f4;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#ffb482;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"110\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#8de5a1;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"165\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#ff9f9b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"220\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#d0bbff;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"275\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#debb9b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"330\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#fab0e4;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"385\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#cfcfcf;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"440\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#fffea3;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"495\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#b9f2f0;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "24006fbcfc549b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reading text from txt file\n",
    "df = stage_data_io_service.run_or_load_stage_data(reload_stage=False)"
   ],
   "id": "e3c3f0fab4cb5dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering and Data Processing for Model Building",
   "id": "be0081a88e90ef2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding Synthetic Replenishment Data for Accurate Calculation of Credit Debt Repayment Stability (CDRS) Ratio\n",
    "\n",
    "Before calculating the Credit Debt Repayment Stability (CDRS) ratio, it is crucial to account for all transactions affecting the available balance, especially replenishments. However, the dataset lacks direct information on replenishment transactions, except for specific cases like reversal transactions. This absence introduces potential inaccuracies when calculating repayment stability.\n",
    "\n",
    "To resolve this issue without altering the data schema by adding new markers, we will synthetically generate missing replenishment transactions. The approach works as follows:\n",
    "\n",
    "1. **Data Grouping and Sorting**: We group the transactions by account number and arrange them in chronological order to establish a clear transaction flow.\n",
    "   \n",
    "2. **Pairwise Transaction Comparison**: After sorting, we compare each transaction with the preceding one. If the available balance of the current transaction exceeds that of the previous one and the previous transaction is not classified as a reversal or address verification, we infer that a replenishment transaction was missing between the two.\n",
    "\n",
    "3. **Synthetic Replenishment Insertion**: For each identified gap, a synthetic replenishment transaction is inserted with a calculated amount and timestamp. This ensures that all changes in the available balance are accurately reflected in the dataset, allowing for a more precise calculation of the CDRS ratio.\n",
    "\n",
    "By synthesizing replenishment data in this way, we can achieve a more accurate and realistic assessment of credit debt \n",
    "repayment behavior without distorting the original dataset's structure.\n"
   ],
   "id": "77dbde257801151f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_skipped_replenishment_transactions(df):\n",
    "    \"\"\"\n",
    "    Adds synthetic replenishment transactions to a DataFrame of financial transactions.\n",
    "\n",
    "    The method processes transaction data for each account, ensuring replenishment transactions \n",
    "    are inserted when missing. It works by grouping transactions by account number, \n",
    "    sorting them chronologically, and checking if a replenishment transaction is needed \n",
    "    between consecutive transactions. If the available balance of a later transaction \n",
    "    is greater than the previous one (and the previous transaction is not a reversal or address verification), \n",
    "    a replenishment transaction is added synthetically.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing transaction data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The original DataFrame with added synthetic replenishment transactions. \n",
    "    The output will be sorted by 'accountNumber' and 'transactionDateTime'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort data by account number and transaction datetime\n",
    "    df = df.sort_values(['accountNumber', 'transactionDateTime']).reset_index(drop=True)\n",
    "\n",
    "    # List to store new replenishment transactions\n",
    "    replenishments = []\n",
    "\n",
    "    # Group transactions by account number\n",
    "    grouped = df.groupby('accountNumber')\n",
    "\n",
    "    # Iterate over each account group\n",
    "    for account, group in grouped:\n",
    "        # Create a replenishment before the first transaction\n",
    "        first_transaction = group.iloc[0]\n",
    "        replenish_row = first_transaction.copy()\n",
    "        replenish_row['transactionDateTime'] = first_transaction['transactionDateTime'] - datetime.timedelta(days=1)\n",
    "        replenish_row['transactionAmount'] = first_transaction['creditLimit']\n",
    "        replenish_row['enteredCVV'] = first_transaction['cardCVV']\n",
    "        replenish_row['transactionType'] = 'REPLENISHMENT'\n",
    "        replenish_row['isFraud'] = False\n",
    "        replenishments.append(replenish_row)\n",
    "\n",
    "        # Process the rest of the transactions\n",
    "        for i in range(1, len(group)):\n",
    "            current_transaction = group.iloc[i]\n",
    "            previous_transaction = group.iloc[i - 1]\n",
    "\n",
    "            # Check if a replenishment is needed between transactions\n",
    "            if (current_transaction['availableMoney'] >= previous_transaction['availableMoney'] and\n",
    "                    previous_transaction['transactionAmount'] != 0 and\n",
    "                    previous_transaction['transactionType'] not in ['REVERSAL', 'ADDRESS_VERIFICATION']):\n",
    "                # Calculate the replenishment datetime\n",
    "                difference = current_transaction['transactionDateTime'] - previous_transaction['transactionDateTime']\n",
    "                replenishment_transaction_date_time = (\n",
    "                    current_transaction['transactionDateTime'] if difference <= datetime.timedelta(0)\n",
    "                    else current_transaction['transactionDateTime'] - difference / 2\n",
    "                )\n",
    "\n",
    "                # Create replenishment transaction\n",
    "                replenish_row = previous_transaction.copy()\n",
    "                replenish_row['transactionDateTime'] = replenishment_transaction_date_time\n",
    "                replenish_row['availableMoney'] = previous_transaction['availableMoney'] - previous_transaction[\n",
    "                    'transactionAmount']\n",
    "                replenish_row['creditDebt'] = previous_transaction['creditDebt'] + previous_transaction[\n",
    "                    'transactionAmount']\n",
    "                replenish_row['transactionAmount'] = current_transaction['availableMoney'] - replenish_row[\n",
    "                    'availableMoney']\n",
    "                replenish_row['enteredCVV'] = previous_transaction['cardCVV']\n",
    "                replenish_row['transactionType'] = 'REPLENISHMENT'\n",
    "                replenish_row['isFraud'] = False\n",
    "\n",
    "                # Append replenishment to the list\n",
    "                replenishments.append(replenish_row)\n",
    "\n",
    "    # Convert replenishments to DataFrame and merge with the original DataFrame\n",
    "    replenishment_df = pd.DataFrame(replenishments)\n",
    "    result_df = pd.concat([df, replenishment_df]).sort_values(['accountNumber', 'transactionDateTime']).reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Example call of the function\n",
    "df = stage_data_io_service.run_or_load_snapshot_data('add_skipped_replenishment_transactions', add_skipped_replenishment_transactions, df, False)\n"
   ],
   "id": "44d08fffc8f6660c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check dimension after adding new rows\n",
    "\n",
    "eda_utils.data_summary(df)"
   ],
   "id": "40ec5e00a7b82e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Credit debt repayment stability ratio (CDRS Ratio)\n",
    "\n",
    "### Overview\n",
    "The primary goal of this analysis is to explore the relationship between a customer's spending and repayment patterns and the likelihood of fraudulent activity (`isFraud`). Specifically, we calculate a **stability coefficient** that reflects how consistently a customer closes their credit debt over time, then analyze how this coefficient correlates with fraud probability.\n",
    "\n",
    "### Key Variables\n",
    "\n",
    "1. **Spending Percentage**: The percentage of the credit limit that was spent between full debt repayments.\n",
    " $$ \\text{Spent Percentage} = \\left( \\frac{\\text{Total Amount Spent}}{\\text{Credit Limit}} \\right) \\times 100 $$\n",
    "\n",
    "2. **Days Between Resets**: The number of days between two consecutive events where the available balance equals the credit limit (i.e., the debt is fully repaid).\n",
    "\n",
    "### Stability ratio\n",
    "\n",
    "To measure customer stability, we consider two factors:\n",
    "- The **average spending percentage** between resets.\n",
    "- The **average number of days** between resets.\n",
    "\n",
    "These two values are normalized using Min-Max normalization to bring them into the range of [0, 1]. The final stability coefficient is computed as:\n",
    "\n",
    "$$ \\text{CDRS Ratio} = 1 - (\\text{Normalized Spending Percentage Between Resets} \\times \\text{Normalized Days Between Resets}) $$\n",
    "\n",
    "This coefficient ranges from 0 to 1, where:\n",
    "- A value closer to **1** indicates a more stable customer (less spending, more frequent repayments).\n",
    "- A value closer to **0** indicates less stable repayment behavior.\n",
    "\n",
    "### Hypothesis\n",
    "We hypothesize that a higher stability coefficient (more stable customers) correlates with a lower probability of fraudulent activity. Conversely, lower stability may indicate higher fraud risk.\n",
    "\n",
    "### Fraud Analysis\n",
    "Once the CDRS Ratio is calculated for each account, we group the accounts into ranges (bins) of stability coefficients and calculate the percentage of fraudulent transactions (`isFraud = True`) in each range. The results are visualized to test the hypothesis.\n",
    "\n",
    "### Conclusion\n",
    "This analysis can help identify patterns between customer behavior and fraud risk, providing valuable insights for improving fraud detection models.\n",
    "\n",
    "### Nice to fix and improve\n",
    "- Investigate anomaly in cdrs-ratio range from 0.222-0.333 -- 0.333-0.444\n",
    "- Improve scaling function (when size of input is 1)\n",
    "- Investigate and fix when normalized `avg_norm_days_between_resets` is NaN (reason might be in scaling function)\n"
   ],
   "id": "ce7d73856825add3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Function for Min-Max normalization\n",
    "def min_max_normalize(values):\n",
    "    \"\"\"\n",
    "    Normalizes a list of values using Min-Max normalization.\n",
    "\n",
    "    Parameters:\n",
    "    values: list or array of numerical values to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    Normalized numpy array of values in range [0, 1].\n",
    "    \"\"\"\n",
    "    if len(values) > 1:\n",
    "        return (np.array(values) - min(values)) / (max(values) - min(values))\n",
    "    else:\n",
    "        return np.array([1])\n",
    "\n",
    "\n",
    "# Function to visualize fraud percentage based on stability coefficient\n",
    "def visualize_fraud_vs_cdrs_ratio(fraud_stats):\n",
    "    \"\"\"\n",
    "    Visualizes the fraud percentage based on cdrs ratio ranges.\n",
    "\n",
    "    Parameters:\n",
    "    fraud_stats: DataFrame containing fraud statistics grouped by cdrs ratio ranges.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fraud_stats.index.astype(str), fraud_stats['fraud_percentage'], marker='o', linestyle='-', color='r')\n",
    "    plt.title('Fraud Percentage vs CDRS Ratio')\n",
    "    plt.xlabel('Credit debt repayment stability ratio Range')\n",
    "    plt.ylabel('Fraud Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_account_stability_stats(account, transactions, reset_indices, stability_data):\n",
    "    days_between_resets = []\n",
    "    spent_percentages = []\n",
    "    # Calculate the amount spent and days between credit limit resets\n",
    "    for i in range(1, len(reset_indices)):\n",
    "        start_idx = (reset_indices[i - 1]) + 1  # not include transaction of replenishment\n",
    "        end_idx = (reset_indices[i]) - 1  # not include transaction of replenishment\n",
    "\n",
    "        # Exclude replenishment transactions from the spending calculation\n",
    "        transaction_slice = transactions.loc[start_idx:end_idx]\n",
    "        non_replenishment_transactions = transaction_slice[\n",
    "            (transaction_slice['transactionType'] != 'REPLENISHMENT') &\n",
    "            (transaction_slice['transactionType'] != 'ADDRESS_VERIFICATION') &\n",
    "            (transaction_slice['transactionType'] != 'REVERSAL')\n",
    "            ]\n",
    "\n",
    "        # Calculate the total amount spent between resets\n",
    "        total_spent = non_replenishment_transactions['transactionAmount'].sum()\n",
    "        credit_limit = transactions.loc[start_idx, 'creditLimit']\n",
    "\n",
    "        spent_percentage = abs((total_spent / credit_limit) * 100)\n",
    "        spent_percentages.append(spent_percentage)\n",
    "\n",
    "        # Calculate the number of days between the two resets\n",
    "        start_date = transactions.loc[start_idx, 'transactionDateTime']\n",
    "        end_date = transactions.loc[end_idx, 'transactionDateTime']\n",
    "        days_between = (end_date - start_date).days\n",
    "        days_between_resets.append(days_between)\n",
    "\n",
    "    # Normalize spent percentages and days between resets\n",
    "    norm_spent_percentages = min_max_normalize(spent_percentages)\n",
    "    norm_days_between_resets = min_max_normalize(days_between_resets)\n",
    "    # Calculate the average normalized values\n",
    "    avg_norm_spent_percentage = np.mean(norm_spent_percentages)\n",
    "    avg_norm_days_between_resets = np.mean(norm_days_between_resets)\n",
    "    # Stability ratio: higher values mean the customer is more stable\n",
    "    cdrs_ratio = 1 - (avg_norm_spent_percentage * avg_norm_days_between_resets)\n",
    "    stability_data.append({\n",
    "        'accountNumber': account,\n",
    "        'creditLimit': transactions.iloc[0].creditLimit,\n",
    "        'avg_norm_spent_percentage': avg_norm_spent_percentage,\n",
    "        'avg_norm_days_between_resets': avg_norm_days_between_resets,\n",
    "        'cdrs_ratio': cdrs_ratio\n",
    "    })\n",
    "\n",
    "def calculate_fraud_stats(df, stability_stats):\n",
    "    # Merge with the original DataFrame on accountNumber\n",
    "    df_merged = df[['accountNumber', 'isFraud']].drop_duplicates().merge(stability_stats, on='accountNumber',\n",
    "                                                                         how='left')\n",
    "    # Create bins for stability coefficient ranges\n",
    "    df_merged['cdrs_range'] = pd.cut(df_merged['cdrs_ratio'], bins=np.linspace(0, 1, 10))\n",
    "    # Calculate fraud percentage in each stability coefficient range\n",
    "    fraud_stats = df_merged.groupby('cdrs_range').agg(\n",
    "        fraud_count=('isFraud', lambda x: (x == True).sum()),\n",
    "        total_count=('isFraud', 'count')\n",
    "    )\n",
    "    fraud_stats['fraud_percentage'] = (fraud_stats['fraud_count'] / fraud_stats['total_count']) * 100\n",
    "    return fraud_stats\n",
    "\n",
    "def calculate_cdrs_ratio(df, credit_cover_threshold):\n",
    "    \"\"\"\n",
    "    Calculates the credit debt repayment stability coefficient for each account and visualizes fraud probability based on the coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    df: DataFrame containing transaction data for various accounts.\n",
    "    credit_cover_threshold (ex. 0.2, 0.7, 1): float, the percentage threshold for considering replenishment transactions to be included in \n",
    "    stability calculations. Only replenishment transactions that result in an available balance exceeding \n",
    "    credit_cover_threshold% of the credit limit will be considered.\n",
    "\n",
    "    Returns:\n",
    "    Updated DataFrame with stability coefficient added, and fraud statistics DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['accountNumber', 'transactionDateTime'])\n",
    "\n",
    "    stability_data = []\n",
    "\n",
    "    # Loop through each account and calculate stability coefficient\n",
    "    for account, transactions in df.groupby('accountNumber'):\n",
    "        transactions = transactions.reset_index(drop=True)\n",
    "\n",
    "        # Find indices where credit limit is reset (repaid based on the X percent threshold)\n",
    "        reset_indices = transactions[\n",
    "            (transactions['transactionType'] == 'REPLENISHMENT') &\n",
    "            ((transactions['transactionAmount'] + transactions['availableMoney']) >= transactions[\n",
    "                'creditLimit'] * credit_cover_threshold)\n",
    "            ].index.tolist()\n",
    "\n",
    "        if len(reset_indices) < 2:\n",
    "            continue  # Skip accounts with insufficient data for calculation. Ratio for these account will be predicted by other model\n",
    "\n",
    "        calculate_account_stability_stats(account, transactions, reset_indices, stability_data)\n",
    "\n",
    "    # Create DataFrame with stability coefficients for each account\n",
    "    stability_stats = pd.DataFrame(stability_data)\n",
    "\n",
    "    fraud_stats = calculate_fraud_stats(df, stability_stats)\n",
    "\n",
    "    # Add stability coefficient to the original DataFrame\n",
    "    df = df.merge(stability_stats, on='accountNumber', how='left')\n",
    "\n",
    "    return df, fraud_stats, stability_stats\n",
    "\n",
    "\n",
    "# Call the function to analyze and visualize\n",
    "df_with_stability, fraud_stats, tuned_stability_stats = calculate_cdrs_ratio(df, credit_cover_threshold=1)\n",
    "visualize_fraud_vs_cdrs_ratio(fraud_stats)"
   ],
   "id": "a8924ff0872e0c8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eda_utils.data_summary(df_with_stability)",
   "id": "825b519b3714c970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting CDRS ratio for account with insufficient replenishments according to threshold\n",
   "id": "d3eb65a9d2205f31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "df = df.sort_values(['accountNumber', 'transactionDateTime'])\n",
    "\n",
    "stability_data = []\n",
    "\n",
    "for account, transactions in df.groupby('accountNumber'):\n",
    "    transactions = transactions.reset_index(drop=True)\n",
    "\n",
    "    reset_indices = transactions[transactions['transactionType'] == 'REPLENISHMENT'].index.tolist()\n",
    "\n",
    "    if len(reset_indices) < 2:\n",
    "        reset_indices = [transactions.head(1).index.item(), transactions.tail(1).index.item()]\n",
    "\n",
    "    calculate_account_stability_stats(account, transactions, reset_indices, stability_data)\n",
    "\n",
    "stability_stats_full = pd.DataFrame(stability_data)\n",
    "stability_stats_full['cdrs_ratio'] = np.nan\n",
    "\n",
    "merged_df = stability_stats_full[['accountNumber']].merge(\n",
    "    tuned_stability_stats[['accountNumber', 'cdrs_ratio']],\n",
    "    on='accountNumber',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "stability_stats_full['cdrs_ratio'] = merged_df['cdrs_ratio']\n",
    "\n",
    "stability_stats_full['cdrs_range'] = pd.cut(stability_stats_full['cdrs_ratio'], bins=np.linspace(0, 1, 10))\n",
    "stability_stats_full\n"
   ],
   "id": "5be1a6096cfaa9d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stability_stats_full.describe()",
   "id": "8ccfb62aec9a4fa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eda_utils.data_summary(stability_stats_full)",
   "id": "3a7fe4fb0dba5db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.heatmap(stability_stats_full[\n",
    "                ['creditLimit', 'cdrs_ratio', 'avg_norm_days_between_resets', 'avg_norm_spent_percentage']].corr(),\n",
    "            cmap=\"Reds\", annot=True)"
   ],
   "id": "4976e15ed603cf35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "both_avg_stats_is_na_accounts = stability_stats_full[(stability_stats_full['avg_norm_days_between_resets'].isnull()) & (\n",
    "    stability_stats_full['avg_norm_spent_percentage'].isnull())]['accountNumber']\n",
    "stab_df_both_avg_stats_is_na_accounts = stability_stats_full[\n",
    "    stability_stats_full['accountNumber'].isin(both_avg_stats_is_na_accounts)]\n",
    "stab_df_both_avg_stats_is_na_accounts"
   ],
   "id": "17b9f2864fc0fbbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gen_df_with_both_invalid_avg_stats = df[df['accountNumber'].isin(both_avg_stats_is_na_accounts)]\n",
    "print(\n",
    "    f'Count of account transactions where both necessary params avg (spent percentage, avg days btw resets) is nan in stability stats: {gen_df_with_both_invalid_avg_stats.shape[0]}')\n",
    "eda_utils.calculate_percentage(gen_df_with_both_invalid_avg_stats.shape[0], df.shape[0])\n",
    "print(\n",
    "    f'Count of frauds transactions in gen dataset by these account numbers: {gen_df_with_both_invalid_avg_stats[gen_df_with_both_invalid_avg_stats[\"isFraud\"] == True].shape[0]}')"
   ],
   "id": "830bc342570104cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.drop(index=gen_df_with_both_invalid_avg_stats.index, inplace=True)\n",
    "stability_stats_full.drop(index=stab_df_both_avg_stats_is_na_accounts.index, inplace=True)\n",
    "eda_utils.data_summary(stability_stats_full)"
   ],
   "id": "b66301d118ddbb53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stab_df_invalid_spent_avg_stats_is_na_accounts = \\\n",
    "    stability_stats_full[stability_stats_full['avg_norm_spent_percentage'].isnull()]['accountNumber']\n",
    "gen_df_with_invalid_spent_avg_stats = df[df['accountNumber'].isin(stab_df_invalid_spent_avg_stats_is_na_accounts)]\n",
    "\n",
    "print(\n",
    "    f'Count of account transactions where avg spent percentage btw resets in nan in stability stats: {gen_df_with_invalid_spent_avg_stats.shape[0]}')\n",
    "eda_utils.calculate_percentage(gen_df_with_invalid_spent_avg_stats.shape[0], df.shape[0])\n",
    "print(\n",
    "    f'Count of frauds transactions in gen dataset by these account numbers: {gen_df_with_invalid_spent_avg_stats[gen_df_with_invalid_spent_avg_stats[\"isFraud\"] == True].shape[0]}')"
   ],
   "id": "6cb09108ced15b38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[df['accountNumber'] == '259711806']",
   "id": "2a355cc0128bc7fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.drop(index=gen_df_with_invalid_spent_avg_stats.index, inplace=True)\n",
    "stability_stats_full.drop(index=stab_df_invalid_spent_avg_stats_is_na_accounts.index, inplace=True)\n",
    "eda_utils.data_summary(stability_stats_full)"
   ],
   "id": "e02af108f3b7c5e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "days_avg_stat_is_na_accounts = stability_stats_full[stability_stats_full['avg_norm_days_between_resets'].isnull()][\n",
    "    'accountNumber']\n",
    "stab_df_days_avg_stats_is_na_accounts = stability_stats_full[\n",
    "    stability_stats_full['accountNumber'].isin(days_avg_stat_is_na_accounts)]\n",
    "stab_df_days_avg_stats_is_na_accounts"
   ],
   "id": "59535582ccaae728"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gen_df_with_invalid_days_avg_stats = df[df['accountNumber'].isin(days_avg_stat_is_na_accounts)]\n",
    "print(\n",
    "    f'Count of account transactions where avg days btw resets in nan in stability stats: {gen_df_with_invalid_days_avg_stats.shape[0]}')\n",
    "eda_utils.calculate_percentage(gen_df_with_invalid_days_avg_stats.shape[0], df.shape[0])\n",
    "print(\n",
    "    f'Count of frauds transactions in gen dataset by these account numbers: {gen_df_with_invalid_days_avg_stats[gen_df_with_invalid_days_avg_stats[\"isFraud\"] == True].shape[0]}')\n",
    "\n",
    "eda_utils.calculate_percentage(gen_df_with_invalid_days_avg_stats[gen_df_with_invalid_days_avg_stats[\"isFraud\"] == True].shape[0],\n",
    "                     df[df['isFraud'] == True].shape[0])"
   ],
   "id": "3706abf92f9890d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[df['accountNumber'] == '155977598']",
   "id": "c6f1fda68b4a94d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.drop(index=gen_df_with_invalid_days_avg_stats.index, inplace=True)\n",
    "stability_stats_full.drop(index=stab_df_days_avg_stats_is_na_accounts.index, inplace=True)\n",
    "eda_utils.data_summary(stability_stats_full)"
   ],
   "id": "c354a09b937c1a5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cdrs_ratio_is_0_accounts = stability_stats_full[stability_stats_full['cdrs_ratio'] == 0]['accountNumber']\n",
    "stab_df_cdrs_ratio_is_0 = stability_stats_full[stability_stats_full['accountNumber'].isin(cdrs_ratio_is_0_accounts)]\n",
    "stab_df_cdrs_ratio_is_0"
   ],
   "id": "a3acbffbd96ec418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gen_df_cdrs_ratio_is_0 = df[df['accountNumber'].isin(cdrs_ratio_is_0_accounts)]\n",
    "print(f'Count of account transactions where cdrs ratio is 0 in stability stats: {gen_df_cdrs_ratio_is_0.shape[0]}')\n",
    "eda_utils.calculate_percentage(gen_df_cdrs_ratio_is_0.shape[0], df.shape[0])\n",
    "print(\n",
    "    f'Count of frauds transactions in gen dataset by these account numbers: {gen_df_cdrs_ratio_is_0[gen_df_cdrs_ratio_is_0[\"isFraud\"] == True].shape[0]}')\n",
    "\n",
    "eda_utils.calculate_percentage(gen_df_cdrs_ratio_is_0[gen_df_cdrs_ratio_is_0[\"isFraud\"] == True].shape[0],\n",
    "                     df[df['isFraud'] == True].shape[0])\n"
   ],
   "id": "e7bbbd4fd99fba0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.drop(index=gen_df_cdrs_ratio_is_0.index, inplace=True)\n",
    "stability_stats_full.drop(index=cdrs_ratio_is_0_accounts.index, inplace=True)\n",
    "eda_utils.data_summary(stability_stats_full)"
   ],
   "id": "dcd865a4be7a5a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stability_stats_full_test_pred_df = stability_stats_full.copy()\n",
    "stability_stats_full_test_pred_df.dropna(inplace=True)\n",
    "cdrs_range_encoder = LabelEncoder()\n",
    "\n",
    "stability_stats_full_test_pred_df['cdrs_range_encoded'] = cdrs_range_encoder.fit_transform(\n",
    "    stability_stats_full_test_pred_df['cdrs_range'])\n",
    "eda_utils.data_summary(stability_stats_full_test_pred_df)\n"
   ],
   "id": "479e2f959e5683bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_stats_full = stability_stats_full_test_pred_df.drop(\n",
    "    columns=['cdrs_range', 'cdrs_range_encoded', 'cdrs_ratio', 'accountNumber'])\n",
    "Y_stats_full = stability_stats_full_test_pred_df['cdrs_range_encoded']\n",
    "\n",
    "X_stats_full_train, X_stats_full_test, Y_stats_full_train, Y_stats_full_test = train_test_split(X_stats_full,\n",
    "                                                                                                Y_stats_full,\n",
    "                                                                                                test_size=0.2,\n",
    "                                                                                                random_state=101)"
   ],
   "id": "954e128e47493de8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cdrs_range_encoded_weights = compute_class_weight('balanced', classes=Y_stats_full_train.unique(), y=Y_stats_full_train)\n",
    "print(f'Cdrs range distribution weights in train dataset {cdrs_range_encoded_weights}')"
   ],
   "id": "51223d39eb443cc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Define the models and their hyperparameters\n",
    "models_dict = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(random_state=101, multi_class='auto'),\n",
    "        \"params\": {\n",
    "            \"solver\": ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "            \"C\": [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        \"model\": GaussianNB(),\n",
    "        \"params\": {\n",
    "            \"var_smoothing\": [1e-09, 1e-08, 1e-07, 1e-06]\n",
    "        }\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"model\": neighbors.KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_neighbors\": [3, 5, 7, 9],\n",
    "            \"weights\": ['uniform', 'distance'],\n",
    "            \"metric\": ['euclidean', 'manhattan']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=101),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": list(range(2,25,1)), \n",
    "            \"min_samples_leaf\": list(range(1,20,1))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ],
   "id": "6f508ed5cd18a407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "def tune_and_evaluate_models(models, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    Tunes each model in the provided dictionary, selects the best model based on accuracy, \n",
    "    and returns the best model with its name and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary where keys are model names and values are dictionaries\n",
    "                       with keys \"model\" and \"params\" for each model.\n",
    "        X_train, X_test: Training and testing features.\n",
    "        Y_train, Y_test: Training and testing labels.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The model with the highest accuracy after tuning.\n",
    "        best_model_name (str): Name of the best model.\n",
    "        best_accuracy (float): Best model's accuracy on the test set.\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "\n",
    "    for model_name, config in models.items():\n",
    "        grid_search = GridSearchCV(config[\"model\"], config[\"params\"], cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "        \n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        pred = best_estimator.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, pred)\n",
    "\n",
    "        print(f'Best parameters for {model_name}: {grid_search.best_params_}')\n",
    "        print(f'Accuracy for {model_name}: {accuracy * 100:.2f}%')\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = best_estimator\n",
    "            best_model_name = model_name\n",
    "\n",
    "    print(f'\\nBest model is {best_model_name} with an accuracy of {best_accuracy * 100:.2f}%')\n",
    "    return best_model, best_model_name, best_accuracy\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    \"\"\"\n",
    "    Saves the best model to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to save.\n",
    "        model_name (str): The name of the model, used for the filename.\n",
    "    \"\"\"\n",
    "    filename = f'{model_name}.pkl'\n",
    "    joblib.dump(model, filename)\n",
    "    print(f'The best model ({model_name}) has been saved as \"{filename}\".')\n",
    "\n",
    "def plot_roc_curve_model(model, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for the model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to plot the ROC curve for.\n",
    "        X_test, Y_test: Testing data.\n",
    "    \"\"\"\n",
    "    # Predict probabilities for the positive class\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_model(model, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix for the model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to plot the confusion matrix for.\n",
    "        X_test, Y_test: Testing data.\n",
    "    \"\"\"\n",
    "    pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(Y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Plots feature importance for models that have a 'feature_importances_' attribute.\n",
    "\n",
    "    Args:\n",
    "        model: The model to plot feature importance for.\n",
    "        feature_names: List of feature names for labeling.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = importances.argsort()[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.bar(range(len(indices)), importances[indices], align=\"center\")\n",
    "        plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"The selected model does not support feature importance plotting.\")\n",
    "\n",
    "\n"
   ],
   "id": "d7851efbce7fdbda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example of how to use these functions\n",
    "# Assume models_dict is defined with the models and parameters you want to tune\n",
    "cdrs_range_best_model, cdrs_range_best_model_name, cdrs_range_model_best_accuracy = tune_and_evaluate_models(models_dict, X_stats_full_train, X_stats_full_test, Y_stats_full_train, Y_stats_full_test)"
   ],
   "id": "536d69517c82966"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_confusion_matrix_model(cdrs_range_best_model, X_stats_full_test, Y_stats_full_test)\n",
    "plot_feature_importance(cdrs_range_best_model, X_stats_full_train.columns)"
   ],
   "id": "5378049ac60a08e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eda_utils.data_summary(stability_stats_full)",
   "id": "77a8e455520e2384"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "empty_cdrs_ratio_accounts = stability_stats_full[stability_stats_full['cdrs_range'].isnull()]\n",
    "data_for_cdrs_range_prediction = empty_cdrs_ratio_accounts.drop(columns=['cdrs_ratio', 'cdrs_range', 'accountNumber'])\n",
    "encoded_predicted_cdrs_range = cdrs_range_best_model.predict(data_for_cdrs_range_prediction)\n",
    "empty_cdrs_ratio_accounts['cdrs_range'] = cdrs_range_encoder.inverse_transform(encoded_predicted_cdrs_range)\n",
    "\n",
    "mean_cdrs_ratio_per_range = stability_stats_full[stability_stats_full['cdrs_range'].notna()].groupby('cdrs_range')['cdrs_ratio'].mean()\n",
    "def fill_na_with_mean(row):\n",
    "    if pd.isna(row['cdrs_ratio']):\n",
    "        return mean_cdrs_ratio_per_range[row['cdrs_range']]\n",
    "    else:\n",
    "        return row['cdrs_ratio']\n",
    "\n",
    "empty_cdrs_ratio_accounts['cdrs_ratio'] = empty_cdrs_ratio_accounts.apply(fill_na_with_mean, axis=1)\n",
    "\n",
    "stability_stats_full.loc[\n",
    "    stability_stats_full['accountNumber'].isin(empty_cdrs_ratio_accounts['accountNumber']),\n",
    "    ['cdrs_range', 'cdrs_ratio']\n",
    "] = empty_cdrs_ratio_accounts[['cdrs_range', 'cdrs_ratio']].values\n",
    "\n",
    "stability_stats_full\n"
   ],
   "id": "2b396c2ebd1eca2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eda_utils.data_summary(stability_stats_full)",
   "id": "acf53c9cca6fe0ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_fraud_vs_cdrs_ratio(calculate_fraud_stats(df, stability_stats_full))",
   "id": "67f297d7ccf4cbac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add cdrs ratio and range to general dataset and drop synthetic replenishment transactions",
   "id": "1b4ed3d257e86d41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cdrs_ratio_data = stability_stats_full[['accountNumber', 'cdrs_ratio', 'cdrs_range']]\n",
    "df = df.merge(cdrs_ratio_data, on='accountNumber', how='left')\n",
    "df = df[df['transactionType'] != 'REPLENISHMENT']\n",
    "df.head()"
   ],
   "id": "286a3615940218f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eda_utils.data_summary(df)",
   "id": "c9ce507dcfcc4fe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating a new feature (MATCH ENTERED CVV OR NOT)",
   "id": "5822c5b313c463ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['cvv_matched_status'] = [True if actual_cvv == entered_cvv else False for actual_cvv, entered_cvv in zip(df['cardCVV'], df['enteredCVV'])]\n",
    "df.drop(columns=['cardCVV', 'enteredCVV'], inplace=True)\n",
    "eda_utils.data_summary(df)"
   ],
   "id": "ee5b957c7233fa45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating a new feature (Address change count)",
   "id": "20dc334667c059e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_address_change_count_column(df):\n",
    "    \"\"\"\n",
    "    Adds a new column 'addressChangeCount' to the DataFrame with the number of address changes\n",
    "    per account up to the time of each transaction.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset containing transactions.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with the new 'addressChangeCount' column.\n",
    "    \"\"\"\n",
    "    # Convert columns with dates to datetime format\n",
    "    df['dateOfLastAddressChange'] = pd.to_datetime(df['dateOfLastAddressChange'])\n",
    "    df['transactionDateTime'] = pd.to_datetime(df['transactionDateTime'])\n",
    "\n",
    "    # Sort by accountNumber and transactionDateTime to ensure correct order\n",
    "    df = df.sort_values(by=['accountNumber', 'transactionDateTime'])\n",
    "\n",
    "    # Initialize a new column for address change count\n",
    "    df['addressChangeCount'] = 0\n",
    "\n",
    "    # Group by account number to process each account's transactions individually\n",
    "    for account, group in df.groupby('accountNumber'):\n",
    "        address_changes = 0\n",
    "        last_change_date = None\n",
    "\n",
    "        # Iterate through each transaction for the account\n",
    "        for idx, row in group.iterrows():\n",
    "            # Check if the current transaction date is after the last address change\n",
    "            if last_change_date is None or row['dateOfLastAddressChange'] > last_change_date:\n",
    "                address_changes += 1\n",
    "                last_change_date = row['dateOfLastAddressChange']\n",
    "\n",
    "            # Set the count of address changes up to the current transaction\n",
    "            df.at[idx, 'addressChangeCount'] = address_changes - 1\n",
    "\n",
    "    return df"
   ],
   "id": "a00121aa83655d4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = add_address_change_count_column(df)",
   "id": "ac3cef2b639256d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eda_utils.data_summary(df)",
   "id": "4e6f3cdaaefd980b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_suspicious_transactions(df, time_threshold_milliseconds):\n",
    "    \"\"\"\n",
    "    Identify suspicious transactions within a specified time threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The transaction dataset, which includes 'accountNumber', 'transactionDateTime', and 'transactionAmount'.\n",
    "    time_threshold_milliseconds (int): The time window in milliseconds within which multiple transactions of the same amount are flagged as suspicious.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The original DataFrame with an additional 'is_suspicious' column.\n",
    "    \"\"\"\n",
    "    # Sort by 'accountNumber' and 'transactionDateTime' to arrange transactions chronologically by account\n",
    "    df = df.sort_values(['accountNumber', 'transactionDateTime']).reset_index(drop=True)\n",
    "    \n",
    "    # Check for duplicate transaction amounts for each account using `transform` to maintain DataFrame shape\n",
    "    is_duplicate_amount = df.groupby('accountNumber')['transactionAmount'].transform(lambda x: x.duplicated())\n",
    "    \n",
    "    # Calculate time difference between consecutive transactions for each account in milliseconds\n",
    "    is_within_time_threshold = df.groupby('accountNumber')['transactionDateTime'].diff() <= pd.Timedelta(time_threshold_milliseconds, unit='milliseconds')\n",
    "    \n",
    "    # Combine conditions: duplicate amount and time difference within threshold\n",
    "    df[\"is_suspicious\"] = is_duplicate_amount & is_within_time_threshold\n",
    "    \n",
    "    return df"
   ],
   "id": "980e9ca9d0463ace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = find_suspicious_transactions(df, time_threshold_milliseconds=3 * 60 * 1000)\n",
    "suspicious_df = df[df['is_suspicious'] == True]\n",
    "\n",
    "print(\"Total number of suspicious transactions: {}\".format(suspicious_df.shape[0]))\n",
    "print(\"Total transaction amount of suspicious transactions: {}$\".format(round(suspicious_df['transactionAmount'].sum(), 2)))\n",
    "eda_utils.data_summary(df)"
   ],
   "id": "362cb80a4d953c88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing df for Model building",
   "id": "c02c6de558519d04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelling_df = df.drop(columns=[\n",
    "    'accountNumber',\n",
    "    'transactionDateTime',\n",
    "    'currentExpDate',\n",
    "    'accountOpenDate',\n",
    "    'dateOfLastAddressChange'\n",
    "])\n",
    "modelling_df.columns"
   ],
   "id": "46d5309e1184575e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cat_cols = ['acqCountry', 'merchantCountryCode', 'posEntryMode', 'posConditionCode', 'merchantCategoryCode', 'transactionType', 'creditLimitRange', 'cdrs_range']",
   "id": "82f687b87056459a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for cat in cat_cols:\n",
    "    modelling_df[cat]= label_encoder.fit_transform(modelling_df[cat])"
   ],
   "id": "44891f303eeaa80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelling_df['cardPresent'] = [1 if cardPresent == True else 0 for cardPresent in modelling_df['cardPresent']]\n",
    "modelling_df['expirationDateKeyInMatch'] = [1 if expirationDateKeyInMatch == True else 0 for expirationDateKeyInMatch in modelling_df['expirationDateKeyInMatch']]\n",
    "modelling_df['isFraud'] = [1 if isFraud == True else 0 for isFraud in modelling_df['isFraud']]\n",
    "modelling_df['cvv_matched_status'] = [1 if cvv_matched_status == True else 0 for cvv_matched_status in modelling_df['cvv_matched_status']]\n",
    "modelling_df['is_suspicious'] = [1 if is_suspicious == True else 0 for is_suspicious in modelling_df['is_suspicious']]"
   ],
   "id": "77d6eaa7193fb3ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_fraud_distribution(df, target_column, title='Transaction Count by isFraud Value'):\n",
    "    \"\"\"\n",
    "    Plot the distribution of transaction counts for fraud and non-fraud transactions.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing transaction data with a binary 'isFraud' column.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f'Recorded transactions: {df.shape}')\n",
    "    # Count the number of transactions where isFraud is True and False\n",
    "    fraud_counts = target_column.value_counts()\n",
    "\n",
    "    # Create a bar plot for the counts\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(hue=fraud_counts.index, y=fraud_counts.values, palette='viridis', legend=False)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('isFraud')\n",
    "    plt.ylabel('Transaction Count')\n",
    "\n",
    "    # Display values on top of each bar\n",
    "    for i, v in enumerate(fraud_counts.values):\n",
    "        plt.text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ],
   "id": "bc54d41f3abcb57e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_fraud_distribution(modelling_df, modelling_df['isFraud'], 'Transaction Count by isFraud Value (without sampling)')",
   "id": "9ec7d8e1b5109c97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target = modelling_df['isFraud']\n",
    "features = modelling_df.drop('isFraud', axis=1)\n",
    "\n",
    "# Performing Under Sampling to balance the dataset and minimize the computation time\n",
    "sampled_modelling_df, sampled_target = RandomUnderSampler(sampling_strategy='majority',\n",
    "                                                          random_state=3).fit_resample(features, target)"
   ],
   "id": "63f4aeacffd10fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_fraud_distribution(sampled_modelling_df, sampled_target, 'Transaction Count by isFraud Value (after undersampling)')",
   "id": "aae04f1c6dc2ea80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "552bd5e95e3284f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stage_data_io_service.write_stage_data(df)",
   "id": "6840eb8805b0193c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "642c5db9528f3353"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
